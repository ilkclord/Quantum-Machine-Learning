{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62YKFfjHNVFU",
        "outputId": "2d72d488-2156-4c8e-e94e-dbbed369a807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane==0.27.0\n",
            "  Downloading PennyLane-0.27.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (1.11.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (3.2.1)\n",
            "Collecting retworkx (from pennylane==0.27.0)\n",
            "  Downloading retworkx-0.13.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane==0.27.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.3.1 (from pennylane==0.27.0)\n",
            "  Downloading autoray-0.6.7-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (5.3.2)\n",
            "Collecting pennylane-lightning>=0.27 (from pennylane==0.27.0)\n",
            "  Downloading PennyLane_Lightning-0.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane==0.27.0) (2.31.0)\n",
            "INFO: pip is looking at multiple versions of pennylane-lightning to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading PennyLane_Lightning-0.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading PennyLane_Lightning-0.32.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading PennyLane_Lightning-0.31.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading PennyLane_Lightning-0.30.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading PennyLane_Lightning-0.29.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading PennyLane_Lightning-0.28.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from pennylane-lightning>=0.27->pennylane==0.27.0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pennylane-lightning>=0.27 (from pennylane==0.27.0)\n",
            "  Downloading PennyLane_Lightning-0.28.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pennylane-lightning to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading PennyLane_Lightning-0.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane==0.27.0) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane==0.27.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane==0.27.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane==0.27.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane==0.27.0) (2023.7.22)\n",
            "Collecting rustworkx==0.13.2 (from retworkx->pennylane==0.27.0)\n",
            "  Downloading rustworkx-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, semantic-version, rustworkx, autoray, retworkx, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.7 ninja-1.11.1.1 pennylane-0.27.0 pennylane-lightning-0.28.0 retworkx-0.13.2 rustworkx-0.13.2 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install pennylane==0.27.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install qiskit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRY512xlNYFb",
        "outputId": "62429083-d2bf-471b-a6ce-285f64d5c8c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-0.45.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting qiskit-terra==0.45.0 (from qiskit)\n",
            "  Downloading qiskit_terra-0.45.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (0.13.2)\n",
            "Requirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (1.23.5)\n",
            "Collecting ply>=3.10 (from qiskit-terra==0.45.0->qiskit)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (1.11.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (1.12)\n",
            "Collecting dill>=0.3 (from qiskit-terra==0.45.0->qiskit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit-terra==0.45.0->qiskit)\n",
            "  Downloading stevedore-5.1.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting symengine<0.10,>=0.9 (from qiskit-terra==0.45.0->qiskit)\n",
            "  Downloading symengine-0.9.2-cp310-cp310-manylinux2010_x86_64.whl (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.45.0->qiskit) (1.16.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit-terra==0.45.0->qiskit)\n",
            "  Downloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.45.0->qiskit) (1.3.0)\n",
            "Installing collected packages: ply, symengine, pbr, dill, stevedore, qiskit-terra, qiskit\n",
            "Successfully installed dill-0.3.7 pbr-6.0.0 ply-3.11 qiskit-0.45.0 qiskit-terra-0.45.0 stevedore-5.1.0 symengine-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-machine-learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vohK15uONhPj",
        "outputId": "07c100b7-479e-40e6-eddb-05a30c2ddc5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit-machine-learning\n",
            "  Downloading qiskit_machine_learning-0.7.0-py3-none-any.whl (96 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/96.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qiskit>=0.44 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (0.45.0)\n",
            "Collecting qiskit-algorithms>=0.2.0 (from qiskit-machine-learning)\n",
            "  Downloading qiskit_algorithms-0.2.1-py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.9/306.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (1.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (1.23.5)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (1.2.2)\n",
            "Collecting fastdtw (from qiskit-machine-learning)\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (67.7.2)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning) (0.3.7)\n",
            "Requirement already satisfied: qiskit-terra==0.45.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.44->qiskit-machine-learning) (0.45.0)\n",
            "Requirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (0.13.2)\n",
            "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (3.11)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (5.1.0)\n",
            "Requirement already satisfied: symengine<0.10,>=0.9 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (0.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (4.5.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->qiskit-machine-learning) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->qiskit-machine-learning) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (6.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.45.0->qiskit>=0.44->qiskit-machine-learning) (1.3.0)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512409 sha256=703264313931861ffe2079a97e9ae32a8a001272a401a013e8450cc7d59bd1a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw, qiskit-algorithms, qiskit-machine-learning\n",
            "Successfully installed fastdtw-0.3.4 qiskit-algorithms-0.2.1 qiskit-machine-learning-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8ew9rEENmqx",
        "outputId": "47888e1b-b438-4efa-8c51-49f5b5565aff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit_aer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7cFngB_NpH6",
        "outputId": "6fef92d8-420c-4666-eea1-9920394eeedd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit_aer\n",
            "  Downloading qiskit_aer-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qiskit>=0.44.0 in /usr/local/lib/python3.10/dist-packages (from qiskit_aer) (0.45.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from qiskit_aer) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit_aer) (1.11.3)\n",
            "Requirement already satisfied: qiskit-terra==0.45.0 in /usr/local/lib/python3.10/dist-packages (from qiskit>=0.44.0->qiskit_aer) (0.45.0)\n",
            "Requirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (0.13.2)\n",
            "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (3.11)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (5.9.5)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (1.12)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (5.1.0)\n",
            "Requirement already satisfied: symengine<0.10,>=0.9 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (0.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (6.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra==0.45.0->qiskit>=0.44.0->qiskit_aer) (1.3.0)\n",
            "Installing collected packages: qiskit_aer\n",
            "Successfully installed qiskit_aer-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3viJeDNhNrLe",
        "outputId": "b6ca5dbe-4492-444e-c74d-a5a79225de65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install qiskit-algorithms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJwazOD1PSxV",
        "outputId": "a8559c0d-5113-4b0f-f473-e1e7f39681fb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit-algorithms in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: qiskit-terra>=0.24 in /usr/local/lib/python3.10/dist-packages (from qiskit-algorithms) (0.45.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from qiskit-algorithms) (1.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit-algorithms) (1.23.5)\n",
            "Requirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (0.13.2)\n",
            "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (3.11)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (5.9.5)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (1.12)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (5.1.0)\n",
            "Requirement already satisfied: symengine<0.10,>=0.9 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (0.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.24->qiskit-algorithms) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-terra>=0.24->qiskit-algorithms) (1.16.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit-terra>=0.24->qiskit-algorithms) (6.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra>=0.24->qiskit-algorithms) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
        "from torch.optim import LBFGS\n",
        "\n",
        "from qiskit import BasicAer\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.utils import algorithm_globals\n",
        "from qiskit.circuit import Parameter\n",
        "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
        "from qiskit_machine_learning.connectors import TorchConnector"
      ],
      "metadata": {
        "id": "6LoAKLFRNtIt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import cat, no_grad, manual_seed\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from torch.nn import (\n",
        "    Module,\n",
        "    Conv2d,\n",
        "    Linear,\n",
        "    Dropout2d,\n",
        "    NLLLoss,\n",
        "    MaxPool2d,\n",
        "    Flatten,\n",
        "    Sequential,\n",
        "    ReLU,\n",
        ")\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "_JUowTF2Nu1T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from qiskit import *\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit.primitives import Sampler as QiskitSampler\n",
        "from qiskit_aer.noise import NoiseModel\n",
        "\n",
        "#from qiskit_ibm_runtime import QiskitRuntimeService, Options, Session, Sampler"
      ],
      "metadata": {
        "id": "Eg6gMNe_Nwyi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "n_feat = 8\n",
        "n_train = 200\n",
        "n_test = 50\n",
        "\n"
      ],
      "metadata": {
        "id": "XXBZkyKKNybs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "\n",
        "def data_load_and_process(dataset, n_feature, classes=[0,1]):\n",
        "\n",
        "    if dataset == 'cifar':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    elif dataset == 'fashion':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "    x_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0\n",
        "\n",
        "    if len(classes) == 2:\n",
        "        train_filter_tf = np.where((y_train == classes[0] ) | (y_train == classes[1] ))\n",
        "        test_filter_tf = np.where((y_test == classes[0] ) | (y_test == classes[1] ))\n",
        "\n",
        "    x_train, y_train = x_train[train_filter_tf], y_train[train_filter_tf]\n",
        "    x_test, y_test = x_test[test_filter_tf], y_test[test_filter_tf]\n",
        "\n",
        "\n",
        "    x_train = tf.image.resize(x_train[:], (256, 1)).numpy()\n",
        "    x_test = tf.image.resize(x_test[:], (256, 1)).numpy()\n",
        "    x_train, x_test = tf.squeeze(x_train).numpy(), tf.squeeze(x_test).numpy()\n",
        "    X_train = PCA(n_feature).fit_transform(x_train)\n",
        "    X_test = PCA(n_feature).fit_transform(x_test)\n",
        "    x_train, x_test = [], []\n",
        "    for x in X_train:\n",
        "        x = (x - x.min()) * (np.pi / (x.max() - x.min()))\n",
        "        x_train.append(x)\n",
        "    for x in X_test:\n",
        "        x = (x - x.min()) * (np.pi / (x.max() - x.min()))\n",
        "        x_test.append(x)\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "GhOTlFsUNz_O"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = data_load_and_process(dataset='cifar', n_feature=n_feat, classes=[0,1])"
      ],
      "metadata": {
        "id": "38aqp8rmOBFq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = X_train[100:200], Y_train[100:200]\n",
        "X_test, Y_test = X_test[50:100], Y_test[50:100]"
      ],
      "metadata": {
        "id": "zo3KegGmOLJ_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_cl = SVC()\n",
        "svc_cl.fit(X_train, Y_train)\n",
        "score_test_cl = svc_cl.score(X_test, Y_test)\n",
        "\n",
        "print(score_test_cl)\n",
        "print(svc_cl.predict(X_test))\n",
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96SgqEX1OS4L",
        "outputId": "227bfaed-3f26-41f9-bdd0-f31c478ddad3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n",
            "[1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0]\n",
            "[1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def new_data(batch_size, X, Y):\n",
        "    X1_new, X2_new, Y_new = [], [], []\n",
        "    for i in range(batch_size):\n",
        "        n, m = np.random.randint(len(X)), np.random.randint(len(X))\n",
        "        X1_new.append(X[n])\n",
        "        X2_new.append(X[m])\n",
        "        if Y[n] == Y[m]:\n",
        "            Y_new.append(1)\n",
        "        else:\n",
        "            Y_new.append(0)\n",
        "    X1_new, X2_new, Y_new = torch.tensor(X1_new).to(torch.float32), torch.tensor(X2_new).to(torch.float32), torch.tensor(Y_new).to(torch.float32)\n",
        "    return X1_new, X2_new, Y_new\n",
        "\n",
        "X1_new_valid, X2_new_valid, Y_new_valid = new_data(10, X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P9AP4hsOg5G",
        "outputId": "a5bfaebb-3080-45b6-9658-5d8bb14add21"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-379f8675d2e7>:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  X1_new, X2_new, Y_new = torch.tensor(X1_new).to(torch.float32), torch.tensor(X2_new).to(torch.float32), torch.tensor(Y_new).to(torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "N_layers = 3\n",
        "\n",
        "# exp(ixZ) gate\n",
        "def exp_Z(x, wires, inverse=False):\n",
        "    if inverse == False:\n",
        "        qml.RZ(-2 * x, wires=wires)\n",
        "    elif inverse == True:\n",
        "        qml.RZ(2 * x, wires=wires)\n",
        "\n",
        "# exp(i(pi - x1)(pi - x2)ZZ) gate\n",
        "def exp_ZZ2(x1, x2, wires, inverse=False):\n",
        "    if inverse == False:\n",
        "        qml.CNOT(wires=wires)\n",
        "        qml.RZ(-2 * (np.pi - x1) * (np.pi - x2), wires=wires[1])\n",
        "        qml.CNOT(wires=wires)\n",
        "    elif inverse == True:\n",
        "        qml.CNOT(wires=wires)\n",
        "        qml.RZ(2 * (np.pi - x1) * (np.pi - x2), wires=wires[1])\n",
        "        qml.CNOT(wires=wires)\n",
        "\n",
        "\n",
        "# Quantum Embedding 1 for model 1 (Conventional ZZ feature embedding)\n",
        "def QuantumEmbedding1(input):\n",
        "    for i in range(N_layers):\n",
        "        for j in range(n_feat):\n",
        "            qml.Hadamard(wires=j)\n",
        "            exp_Z(input[j], wires=j)\n",
        "        for k in range(n_feat-1):\n",
        "            exp_ZZ2(input[k], input[k+1], wires=[k,k+1])\n",
        "        exp_ZZ2(input[n_feat-1], input[0], wires=[n_feat-1, 0])\n",
        "\n",
        "def QuantumEmbedding1_inverse(input):\n",
        "    for i in range(N_layers):\n",
        "        exp_ZZ2(input[n_feat-1], input[0], wires=[n_feat-1, 0], inverse=True)\n",
        "        for k in reversed(range(n_feat-1)):\n",
        "            exp_ZZ2(input[k], input[k+1], wires=[k,k+1], inverse=True)\n",
        "        qml.Barrier()\n",
        "        for j in range(n_feat):\n",
        "            exp_Z(input[j], wires=j, inverse=True)\n",
        "            qml.Hadamard(wires=j)\n",
        "\n",
        "dev = qml.device('default.qubit', wires=n_feat)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def circuit1(inputs):\n",
        "    QuantumEmbedding1(inputs[0:n_feat])\n",
        "    QuantumEmbedding1_inverse(inputs[n_feat:2 * n_feat])\n",
        "    return qml.probs(wires=range(n_feat))"
      ],
      "metadata": {
        "id": "-LSfCuLwOk2P"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class Model1_Fidelity(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.qlayer1 = qml.qnn.TorchLayer(circuit1, weight_shapes={})\n",
        "        self.linear_relu_stack1 = nn.Sequential(\n",
        "            nn.Linear(n_feat, 2 * n_feat),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * n_feat, 2 * n_feat),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * n_feat, n_feat)\n",
        "        )\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.linear_relu_stack1(x1)\n",
        "        x2 = self.linear_relu_stack1(x2)\n",
        "        x = torch.concat([x1, x2], 1)\n",
        "        x = self.qlayer1(x)\n",
        "        return x[:,0]\n",
        "\n",
        "model = Model1_Fidelity()"
      ],
      "metadata": {
        "id": "MypQv0E8Omux"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model, optimizer, and loss function\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "train_loss=[]\n",
        "# Start training\n",
        "\n",
        "loss_list = []  # Store loss history\n",
        "model.train()  # Set model to training mode\n",
        "\n",
        "iterations = 1000\n",
        "for it in range(iterations):\n",
        "    X1_batch, X2_batch, Y_batch = new_data(10, X_train, Y_train)\n",
        "    pred = model(X1_batch, X2_batch)\n",
        "    loss = loss_fn(pred, Y_batch)\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if it % 10 == 0:\n",
        "        print(f\"Iterations: {it} Loss: {loss.item()}\")\n",
        "        with torch.no_grad():\n",
        "            pred_validation = model(X1_new_valid, X2_new_valid)\n",
        "            loss_validation = loss_fn(pred_validation, Y_new_valid)\n",
        "            print(f\"Validation Loss: {loss_validation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y82YwHEOojF",
        "outputId": "5c558a55-3af7-4da4-a722-cc3e63764767"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations: 0 Loss: 0.3538007438182831\n",
            "Validation Loss: 0.3247227072715759\n",
            "Iterations: 10 Loss: 0.19733193516731262\n",
            "Validation Loss: 0.31857746839523315\n",
            "Iterations: 20 Loss: 0.2959960997104645\n",
            "Validation Loss: 0.2676076591014862\n",
            "Iterations: 30 Loss: 0.1314321905374527\n",
            "Validation Loss: 0.26757484674453735\n",
            "Iterations: 40 Loss: 0.2571570575237274\n",
            "Validation Loss: 0.24723251163959503\n",
            "Iterations: 50 Loss: 0.10423173755407333\n",
            "Validation Loss: 0.253164142370224\n",
            "Iterations: 60 Loss: 0.21804404258728027\n",
            "Validation Loss: 0.23841774463653564\n",
            "Iterations: 70 Loss: 0.12743525207042694\n",
            "Validation Loss: 0.20423217117786407\n",
            "Iterations: 80 Loss: 0.15727414190769196\n",
            "Validation Loss: 0.19925811886787415\n",
            "Iterations: 90 Loss: 0.08498872816562653\n",
            "Validation Loss: 0.1830969899892807\n",
            "Iterations: 100 Loss: 0.06490250676870346\n",
            "Validation Loss: 0.1643413007259369\n",
            "Iterations: 110 Loss: 0.03797463700175285\n",
            "Validation Loss: 0.15786737203598022\n",
            "Iterations: 120 Loss: 0.057341933250427246\n",
            "Validation Loss: 0.14427490532398224\n",
            "Iterations: 130 Loss: 0.01256348192691803\n",
            "Validation Loss: 0.13618692755699158\n",
            "Iterations: 140 Loss: 0.06710532307624817\n",
            "Validation Loss: 0.11876432597637177\n",
            "Iterations: 150 Loss: 0.017368054017424583\n",
            "Validation Loss: 0.11153267323970795\n",
            "Iterations: 160 Loss: 0.009195059537887573\n",
            "Validation Loss: 0.10795937478542328\n",
            "Iterations: 170 Loss: 0.011110188439488411\n",
            "Validation Loss: 0.10733760893344879\n",
            "Iterations: 180 Loss: 0.017060762271285057\n",
            "Validation Loss: 0.10478869825601578\n",
            "Iterations: 190 Loss: 0.019466813653707504\n",
            "Validation Loss: 0.09610433131456375\n",
            "Iterations: 200 Loss: 0.03073435090482235\n",
            "Validation Loss: 0.08994384855031967\n",
            "Iterations: 210 Loss: 0.007933746092021465\n",
            "Validation Loss: 0.08957570046186447\n",
            "Iterations: 220 Loss: 0.022741906344890594\n",
            "Validation Loss: 0.08255543559789658\n",
            "Iterations: 230 Loss: 0.017542902380228043\n",
            "Validation Loss: 0.08111357688903809\n",
            "Iterations: 240 Loss: 0.009383738040924072\n",
            "Validation Loss: 0.08148006349802017\n",
            "Iterations: 250 Loss: 0.0046494086273014545\n",
            "Validation Loss: 0.07559169083833694\n",
            "Iterations: 260 Loss: 0.010913919657468796\n",
            "Validation Loss: 0.07192741334438324\n",
            "Iterations: 270 Loss: 0.004936897195875645\n",
            "Validation Loss: 0.062328826636075974\n",
            "Iterations: 280 Loss: 0.024230269715189934\n",
            "Validation Loss: 0.0666375681757927\n",
            "Iterations: 290 Loss: 0.004419529810547829\n",
            "Validation Loss: 0.06877397000789642\n",
            "Iterations: 300 Loss: 0.00225237850099802\n",
            "Validation Loss: 0.06578084081411362\n",
            "Iterations: 310 Loss: 0.005591487977653742\n",
            "Validation Loss: 0.05772131681442261\n",
            "Iterations: 320 Loss: 0.0016679720720276237\n",
            "Validation Loss: 0.05661696940660477\n",
            "Iterations: 330 Loss: 0.01742956042289734\n",
            "Validation Loss: 0.056787073612213135\n",
            "Iterations: 340 Loss: 0.00888010673224926\n",
            "Validation Loss: 0.055425167083740234\n",
            "Iterations: 350 Loss: 0.009265939705073833\n",
            "Validation Loss: 0.04920689016580582\n",
            "Iterations: 360 Loss: 0.007181079592555761\n",
            "Validation Loss: 0.04146364703774452\n",
            "Iterations: 370 Loss: 0.0012446658220142126\n",
            "Validation Loss: 0.04252922162413597\n",
            "Iterations: 380 Loss: 0.0034541445784270763\n",
            "Validation Loss: 0.034174803644418716\n",
            "Iterations: 390 Loss: 0.007600662298500538\n",
            "Validation Loss: 0.03698693960905075\n",
            "Iterations: 400 Loss: 0.0024949500802904367\n",
            "Validation Loss: 0.039679475128650665\n",
            "Iterations: 410 Loss: 0.0026897445786744356\n",
            "Validation Loss: 0.035792961716651917\n",
            "Iterations: 420 Loss: 0.0024686914402991533\n",
            "Validation Loss: 0.03908482939004898\n",
            "Iterations: 430 Loss: 0.0024913533125072718\n",
            "Validation Loss: 0.039323754608631134\n",
            "Iterations: 440 Loss: 0.0042389086447656155\n",
            "Validation Loss: 0.04017942026257515\n",
            "Iterations: 450 Loss: 0.00822135154157877\n",
            "Validation Loss: 0.03578149527311325\n",
            "Iterations: 460 Loss: 0.001163302338682115\n",
            "Validation Loss: 0.03519748896360397\n",
            "Iterations: 470 Loss: 0.003753099124878645\n",
            "Validation Loss: 0.03271126747131348\n",
            "Iterations: 480 Loss: 0.006852348335087299\n",
            "Validation Loss: 0.032288577407598495\n",
            "Iterations: 490 Loss: 0.006148898508399725\n",
            "Validation Loss: 0.02646934613585472\n",
            "Iterations: 500 Loss: 0.020001154392957687\n",
            "Validation Loss: 0.03034774586558342\n",
            "Iterations: 510 Loss: 0.0038326322101056576\n",
            "Validation Loss: 0.024246301501989365\n",
            "Iterations: 520 Loss: 0.0002964666055049747\n",
            "Validation Loss: 0.024196837097406387\n",
            "Iterations: 530 Loss: 0.0024260999634861946\n",
            "Validation Loss: 0.025240426883101463\n",
            "Iterations: 540 Loss: 0.003853070782497525\n",
            "Validation Loss: 0.028980161994695663\n",
            "Iterations: 550 Loss: 0.0038832600694149733\n",
            "Validation Loss: 0.03185521811246872\n",
            "Iterations: 560 Loss: 0.0008358623017556965\n",
            "Validation Loss: 0.02933148480951786\n",
            "Iterations: 570 Loss: 0.003107031574472785\n",
            "Validation Loss: 0.026767244562506676\n",
            "Iterations: 580 Loss: 0.010428272187709808\n",
            "Validation Loss: 0.02430243231356144\n",
            "Iterations: 590 Loss: 0.002018687082454562\n",
            "Validation Loss: 0.025813201442360878\n",
            "Iterations: 600 Loss: 0.00485115684568882\n",
            "Validation Loss: 0.023848386481404305\n",
            "Iterations: 610 Loss: 0.0020017747301608324\n",
            "Validation Loss: 0.027474859729409218\n",
            "Iterations: 620 Loss: 0.0043013934046030045\n",
            "Validation Loss: 0.02539834752678871\n",
            "Iterations: 630 Loss: 0.00262644374743104\n",
            "Validation Loss: 0.02437792345881462\n",
            "Iterations: 640 Loss: 0.0016655015060678124\n",
            "Validation Loss: 0.023008564487099648\n",
            "Iterations: 650 Loss: 0.001428332761861384\n",
            "Validation Loss: 0.022124454379081726\n",
            "Iterations: 660 Loss: 0.004672834184020758\n",
            "Validation Loss: 0.023188695311546326\n",
            "Iterations: 670 Loss: 0.0025233400519937277\n",
            "Validation Loss: 0.02219098061323166\n",
            "Iterations: 680 Loss: 0.00927991233766079\n",
            "Validation Loss: 0.02112726867198944\n",
            "Iterations: 690 Loss: 0.0042500062845647335\n",
            "Validation Loss: 0.02132968232035637\n",
            "Iterations: 700 Loss: 0.010785980150103569\n",
            "Validation Loss: 0.022629262879490852\n",
            "Iterations: 710 Loss: 0.002015485893934965\n",
            "Validation Loss: 0.018991712480783463\n",
            "Iterations: 720 Loss: 0.002291242592036724\n",
            "Validation Loss: 0.021451255306601524\n",
            "Iterations: 730 Loss: 0.002268483629450202\n",
            "Validation Loss: 0.02042330428957939\n",
            "Iterations: 740 Loss: 0.0028551232535392046\n",
            "Validation Loss: 0.01802106387913227\n",
            "Iterations: 750 Loss: 0.0018226004904136062\n",
            "Validation Loss: 0.017410732805728912\n",
            "Iterations: 760 Loss: 0.0032533437479287386\n",
            "Validation Loss: 0.019588157534599304\n",
            "Iterations: 770 Loss: 0.0010620279936119914\n",
            "Validation Loss: 0.01775047741830349\n",
            "Iterations: 780 Loss: 0.003844511229544878\n",
            "Validation Loss: 0.01618841476738453\n",
            "Iterations: 790 Loss: 0.0009811536874622107\n",
            "Validation Loss: 0.015433941967785358\n",
            "Iterations: 800 Loss: 0.0038902834057807922\n",
            "Validation Loss: 0.015315885655581951\n",
            "Iterations: 810 Loss: 0.0045092543587088585\n",
            "Validation Loss: 0.015587806701660156\n",
            "Iterations: 820 Loss: 0.0031995580065995455\n",
            "Validation Loss: 0.017857223749160767\n",
            "Iterations: 830 Loss: 0.003134519560262561\n",
            "Validation Loss: 0.017182594165205956\n",
            "Iterations: 840 Loss: 0.0006695110932923853\n",
            "Validation Loss: 0.0175388865172863\n",
            "Iterations: 850 Loss: 0.004041764419525862\n",
            "Validation Loss: 0.016048507764935493\n",
            "Iterations: 860 Loss: 0.003429752541705966\n",
            "Validation Loss: 0.01767110824584961\n",
            "Iterations: 870 Loss: 0.0044678980484604836\n",
            "Validation Loss: 0.01666554994881153\n",
            "Iterations: 880 Loss: 0.0017761830240488052\n",
            "Validation Loss: 0.017751608043909073\n",
            "Iterations: 890 Loss: 0.0015480152796953917\n",
            "Validation Loss: 0.016184668987989426\n",
            "Iterations: 900 Loss: 0.001636290573514998\n",
            "Validation Loss: 0.014980360865592957\n",
            "Iterations: 910 Loss: 0.003444779198616743\n",
            "Validation Loss: 0.014607439748942852\n",
            "Iterations: 920 Loss: 0.0034053842537105083\n",
            "Validation Loss: 0.015117627568542957\n",
            "Iterations: 930 Loss: 0.006052403245121241\n",
            "Validation Loss: 0.01462048850953579\n",
            "Iterations: 940 Loss: 0.0015346517320722342\n",
            "Validation Loss: 0.014556093141436577\n",
            "Iterations: 950 Loss: 0.0041317930445075035\n",
            "Validation Loss: 0.014810867607593536\n",
            "Iterations: 960 Loss: 0.00720538105815649\n",
            "Validation Loss: 0.013806921429932117\n",
            "Iterations: 970 Loss: 0.00405783299356699\n",
            "Validation Loss: 0.013998243026435375\n",
            "Iterations: 980 Loss: 0.0019735274836421013\n",
            "Validation Loss: 0.012771030887961388\n",
            "Iterations: 990 Loss: 0.01279396377503872\n",
            "Validation Loss: 0.013320240192115307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXUe7mvIOqap",
        "outputId": "f4002062-b019-4131-f53e-dda1ec03ff0a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3538007438182831,\n",
              " 0.3924054801464081,\n",
              " 0.3243235945701599,\n",
              " 0.18863967061042786,\n",
              " 0.18470576405525208,\n",
              " 0.34292906522750854,\n",
              " 0.1707036942243576,\n",
              " 0.3166450262069702,\n",
              " 0.19814491271972656,\n",
              " 0.26679545640945435,\n",
              " 0.19733193516731262,\n",
              " 0.1295357197523117,\n",
              " 0.23034048080444336,\n",
              " 0.13215366005897522,\n",
              " 0.19961991906166077,\n",
              " 0.40263599157333374,\n",
              " 0.2863457500934601,\n",
              " 0.26619452238082886,\n",
              " 0.21097950637340546,\n",
              " 0.21708670258522034,\n",
              " 0.2959960997104645,\n",
              " 0.1867673546075821,\n",
              " 0.275863379240036,\n",
              " 0.2792350947856903,\n",
              " 0.2864401936531067,\n",
              " 0.3848296105861664,\n",
              " 0.11034627258777618,\n",
              " 0.05375928431749344,\n",
              " 0.0790751501917839,\n",
              " 0.28390204906463623,\n",
              " 0.1314321905374527,\n",
              " 0.3105277121067047,\n",
              " 0.2568642795085907,\n",
              " 0.14816921949386597,\n",
              " 0.11600116640329361,\n",
              " 0.17366959154605865,\n",
              " 0.12897303700447083,\n",
              " 0.17744624614715576,\n",
              " 0.106911800801754,\n",
              " 0.23398251831531525,\n",
              " 0.2571570575237274,\n",
              " 0.1013437882065773,\n",
              " 0.19453886151313782,\n",
              " 0.024474134668707848,\n",
              " 0.15195825695991516,\n",
              " 0.2186061143875122,\n",
              " 0.205588698387146,\n",
              " 0.13004127144813538,\n",
              " 0.16553853452205658,\n",
              " 0.22659945487976074,\n",
              " 0.10423173755407333,\n",
              " 0.05248625949025154,\n",
              " 0.16784413158893585,\n",
              " 0.02662932500243187,\n",
              " 0.17436867952346802,\n",
              " 0.11950857937335968,\n",
              " 0.22062630951404572,\n",
              " 0.09486935287714005,\n",
              " 0.06261556595563889,\n",
              " 0.12109694629907608,\n",
              " 0.21804404258728027,\n",
              " 0.09816382825374603,\n",
              " 0.101490318775177,\n",
              " 0.2589542865753174,\n",
              " 0.10513181984424591,\n",
              " 0.010506784543395042,\n",
              " 0.055670373141765594,\n",
              " 0.056087542325258255,\n",
              " 0.05791159346699715,\n",
              " 0.034706879407167435,\n",
              " 0.12743525207042694,\n",
              " 0.07722172886133194,\n",
              " 0.07238414138555527,\n",
              " 0.13394500315189362,\n",
              " 0.005268707871437073,\n",
              " 0.0771978348493576,\n",
              " 0.03660374879837036,\n",
              " 0.12794601917266846,\n",
              " 0.18073037266731262,\n",
              " 0.1281995326280594,\n",
              " 0.15727414190769196,\n",
              " 0.0437917560338974,\n",
              " 0.03756087273359299,\n",
              " 0.03951051086187363,\n",
              " 0.01672663912177086,\n",
              " 0.12578323483467102,\n",
              " 0.028410743921995163,\n",
              " 0.020025160163640976,\n",
              " 0.14309759438037872,\n",
              " 0.12268541753292084,\n",
              " 0.08498872816562653,\n",
              " 0.09410594403743744,\n",
              " 0.036664288491010666,\n",
              " 0.11877711862325668,\n",
              " 0.10728172957897186,\n",
              " 0.051825691014528275,\n",
              " 0.025578629225492477,\n",
              " 0.09772594273090363,\n",
              " 0.023106466978788376,\n",
              " 0.03229541331529617,\n",
              " 0.06490250676870346,\n",
              " 0.00968756154179573,\n",
              " 0.0058340998366475105,\n",
              " 0.025498047471046448,\n",
              " 0.02973569929599762,\n",
              " 0.044442594051361084,\n",
              " 0.04948361963033676,\n",
              " 0.011182591319084167,\n",
              " 0.021271031349897385,\n",
              " 0.008679470047354698,\n",
              " 0.03797463700175285,\n",
              " 0.0152975395321846,\n",
              " 0.05940640717744827,\n",
              " 0.025266295298933983,\n",
              " 0.017629383131861687,\n",
              " 0.022883836179971695,\n",
              " 0.029638703912496567,\n",
              " 0.015455560758709908,\n",
              " 0.060213588178157806,\n",
              " 0.007177302148193121,\n",
              " 0.057341933250427246,\n",
              " 0.12148328125476837,\n",
              " 0.026983922347426414,\n",
              " 0.03769434988498688,\n",
              " 0.02919173240661621,\n",
              " 0.03239384666085243,\n",
              " 0.02437705174088478,\n",
              " 0.08295789361000061,\n",
              " 0.05483323335647583,\n",
              " 0.023548459634184837,\n",
              " 0.01256348192691803,\n",
              " 0.028434108942747116,\n",
              " 0.009450848214328289,\n",
              " 0.06315748393535614,\n",
              " 0.02337680570781231,\n",
              " 0.021252427250146866,\n",
              " 0.0694543793797493,\n",
              " 0.012238699942827225,\n",
              " 0.0268421433866024,\n",
              " 0.07187367230653763,\n",
              " 0.06710532307624817,\n",
              " 0.14446839690208435,\n",
              " 0.05714220926165581,\n",
              " 0.058327604085206985,\n",
              " 0.0165787972509861,\n",
              " 0.030363643541932106,\n",
              " 0.030383765697479248,\n",
              " 0.018727872520685196,\n",
              " 0.025214914232492447,\n",
              " 0.016600260511040688,\n",
              " 0.017368054017424583,\n",
              " 0.01468455046415329,\n",
              " 0.034966085106134415,\n",
              " 0.030947962775826454,\n",
              " 0.02727481722831726,\n",
              " 0.0068352073431015015,\n",
              " 0.03478926792740822,\n",
              " 0.07386797666549683,\n",
              " 0.017703859135508537,\n",
              " 0.009856075048446655,\n",
              " 0.009195059537887573,\n",
              " 0.017522860318422318,\n",
              " 0.02126927487552166,\n",
              " 0.027964571490883827,\n",
              " 0.020852109417319298,\n",
              " 0.014451255090534687,\n",
              " 0.012483170256018639,\n",
              " 0.004735011141747236,\n",
              " 0.01603752002120018,\n",
              " 0.03258533030748367,\n",
              " 0.011110188439488411,\n",
              " 0.07514219731092453,\n",
              " 0.04471071809530258,\n",
              " 0.02992771565914154,\n",
              " 0.043838124722242355,\n",
              " 0.014503071084618568,\n",
              " 0.02534158155322075,\n",
              " 0.02798011340200901,\n",
              " 0.008362485095858574,\n",
              " 0.019290614873170853,\n",
              " 0.017060762271285057,\n",
              " 0.0323994904756546,\n",
              " 0.0386747345328331,\n",
              " 0.025626521557569504,\n",
              " 0.028624389320611954,\n",
              " 0.039526116102933884,\n",
              " 0.008166184648871422,\n",
              " 0.055226705968379974,\n",
              " 0.02287549339234829,\n",
              " 0.013931946828961372,\n",
              " 0.019466813653707504,\n",
              " 0.01592090167105198,\n",
              " 0.009756963700056076,\n",
              " 0.0008107904577627778,\n",
              " 0.042292721569538116,\n",
              " 0.027995552867650986,\n",
              " 0.046655915677547455,\n",
              " 0.010071011260151863,\n",
              " 0.04072194546461105,\n",
              " 0.017785074189305305,\n",
              " 0.03073435090482235,\n",
              " 0.0028198487125337124,\n",
              " 0.004700811579823494,\n",
              " 0.01876496523618698,\n",
              " 0.022701317444443703,\n",
              " 0.0021951019298285246,\n",
              " 0.004833550658077002,\n",
              " 0.058147579431533813,\n",
              " 0.03571349382400513,\n",
              " 0.006613043136894703,\n",
              " 0.007933746092021465,\n",
              " 0.016598127782344818,\n",
              " 0.0016932033468037844,\n",
              " 0.0025302059948444366,\n",
              " 0.011725530959665775,\n",
              " 0.0045863064005970955,\n",
              " 0.010507849976420403,\n",
              " 0.018808642402291298,\n",
              " 0.007715540938079357,\n",
              " 0.03552176058292389,\n",
              " 0.022741906344890594,\n",
              " 0.013844162225723267,\n",
              " 0.014186685904860497,\n",
              " 0.03326389938592911,\n",
              " 0.0018951529636979103,\n",
              " 0.008185023441910744,\n",
              " 0.016966216266155243,\n",
              " 0.003425769042223692,\n",
              " 0.05409040302038193,\n",
              " 0.017313597723841667,\n",
              " 0.017542902380228043,\n",
              " 0.013317873701453209,\n",
              " 0.03867238387465477,\n",
              " 0.005448066163808107,\n",
              " 0.006548636592924595,\n",
              " 0.004729835782200098,\n",
              " 0.007611789740622044,\n",
              " 0.008888961747288704,\n",
              " 0.012498022057116032,\n",
              " 0.015975195914506912,\n",
              " 0.009383738040924072,\n",
              " 0.02269011177122593,\n",
              " 0.03163350000977516,\n",
              " 0.030239194631576538,\n",
              " 0.006599111948162317,\n",
              " 0.011845549568533897,\n",
              " 0.0030044238083064556,\n",
              " 0.00902789831161499,\n",
              " 0.008367294445633888,\n",
              " 0.011474273167550564,\n",
              " 0.0046494086273014545,\n",
              " 0.031802766025066376,\n",
              " 0.008647684939205647,\n",
              " 0.02496771514415741,\n",
              " 0.028077473863959312,\n",
              " 0.029076043516397476,\n",
              " 0.023312263190746307,\n",
              " 0.013268379494547844,\n",
              " 0.01026315987110138,\n",
              " 0.01382880937308073,\n",
              " 0.010913919657468796,\n",
              " 0.0246584415435791,\n",
              " 0.004892241209745407,\n",
              " 0.008596695028245449,\n",
              " 0.020354310050606728,\n",
              " 0.013298717327415943,\n",
              " 0.01576056517660618,\n",
              " 0.01590091548860073,\n",
              " 0.017785638570785522,\n",
              " 0.011355731636285782,\n",
              " 0.004936897195875645,\n",
              " 0.0051616341806948185,\n",
              " 0.04102625697851181,\n",
              " 0.0012190754059702158,\n",
              " 0.004431671462953091,\n",
              " 0.004938778933137655,\n",
              " 0.015012333169579506,\n",
              " 0.029140880331397057,\n",
              " 0.008547628298401833,\n",
              " 0.003096760017797351,\n",
              " 0.024230269715189934,\n",
              " 0.011892243288457394,\n",
              " 0.009626994840800762,\n",
              " 0.01577964797616005,\n",
              " 0.00607529329136014,\n",
              " 0.022861506789922714,\n",
              " 0.0029950537718832493,\n",
              " 0.008700070902705193,\n",
              " 0.017707346007227898,\n",
              " 0.004022843204438686,\n",
              " 0.004419529810547829,\n",
              " 0.005877825431525707,\n",
              " 0.013282345607876778,\n",
              " 0.023507745936512947,\n",
              " 0.004324278794229031,\n",
              " 0.00436182226985693,\n",
              " 0.007834630087018013,\n",
              " 0.015076136216521263,\n",
              " 0.005426667630672455,\n",
              " 0.0019266356248408556,\n",
              " 0.00225237850099802,\n",
              " 0.007220184896141291,\n",
              " 0.013568622060120106,\n",
              " 0.002486732555553317,\n",
              " 0.008937961421906948,\n",
              " 0.020367098972201347,\n",
              " 0.0011817737249657512,\n",
              " 0.00243563880212605,\n",
              " 0.006866891868412495,\n",
              " 0.0055892085656523705,\n",
              " 0.005591487977653742,\n",
              " 0.005546858534216881,\n",
              " 0.0035855728201568127,\n",
              " 0.006928150542080402,\n",
              " 0.004774077795445919,\n",
              " 0.001771658076904714,\n",
              " 0.008697746321558952,\n",
              " 0.014155171811580658,\n",
              " 0.001705061993561685,\n",
              " 0.004817068576812744,\n",
              " 0.0016679720720276237,\n",
              " 0.012249497696757317,\n",
              " 0.00274935201741755,\n",
              " 0.004618164151906967,\n",
              " 0.005599748808890581,\n",
              " 0.0030578847508877516,\n",
              " 0.019166702404618263,\n",
              " 0.0026621173601597548,\n",
              " 0.004806041717529297,\n",
              " 0.013071808032691479,\n",
              " 0.01742956042289734,\n",
              " 0.006406431086361408,\n",
              " 0.018285904079675674,\n",
              " 0.022596878930926323,\n",
              " 0.005959766451269388,\n",
              " 0.007811108138412237,\n",
              " 0.0032278888393193483,\n",
              " 0.01716887205839157,\n",
              " 0.017799699679017067,\n",
              " 0.007954707369208336,\n",
              " 0.00888010673224926,\n",
              " 0.025826241821050644,\n",
              " 0.0034237962681800127,\n",
              " 0.013138771057128906,\n",
              " 0.00186823564581573,\n",
              " 0.0007946596597321332,\n",
              " 0.009079324081540108,\n",
              " 0.005289184395223856,\n",
              " 0.005074640270322561,\n",
              " 0.009083213284611702,\n",
              " 0.009265939705073833,\n",
              " 0.010477582924067974,\n",
              " 0.008191285654902458,\n",
              " 0.005247293971478939,\n",
              " 0.005173633340746164,\n",
              " 0.018628859892487526,\n",
              " 0.016567818820476532,\n",
              " 0.005894137546420097,\n",
              " 0.005737088620662689,\n",
              " 0.003796260105445981,\n",
              " 0.007181079592555761,\n",
              " 0.0029600488487631083,\n",
              " 0.00466175889596343,\n",
              " 0.017657361924648285,\n",
              " 0.010086831636726856,\n",
              " 0.009218024089932442,\n",
              " 0.005913317669183016,\n",
              " 0.005731109529733658,\n",
              " 0.007406248711049557,\n",
              " 0.002864611567929387,\n",
              " 0.0012446658220142126,\n",
              " 0.0018872501095756888,\n",
              " 0.008556726388633251,\n",
              " 0.001625514356419444,\n",
              " 0.03571707755327225,\n",
              " 0.0339941531419754,\n",
              " 0.009049581363797188,\n",
              " 0.007814863696694374,\n",
              " 0.00500244926661253,\n",
              " 0.00498309638351202,\n",
              " 0.0034541445784270763,\n",
              " 0.009077274240553379,\n",
              " 0.012854218482971191,\n",
              " 0.0029764112550765276,\n",
              " 0.006002741400152445,\n",
              " 0.0054935733787715435,\n",
              " 0.008245249278843403,\n",
              " 0.013419827446341515,\n",
              " 0.005765343550592661,\n",
              " 0.0070769572630524635,\n",
              " 0.007600662298500538,\n",
              " 0.002426705788820982,\n",
              " 0.0019703006837517023,\n",
              " 0.0010435194708406925,\n",
              " 0.004491864703595638,\n",
              " 0.008097016252577305,\n",
              " 0.00593107845634222,\n",
              " 0.00545084523037076,\n",
              " 0.029409175738692284,\n",
              " 0.0071043819189071655,\n",
              " 0.0024949500802904367,\n",
              " 0.009228399954736233,\n",
              " 0.011013749055564404,\n",
              " 0.017552126199007034,\n",
              " 0.010165126994252205,\n",
              " 0.0007907346007414162,\n",
              " 0.0022819177247583866,\n",
              " 0.008588620461523533,\n",
              " 0.013250945135951042,\n",
              " 0.0030318466015160084,\n",
              " 0.0026897445786744356,\n",
              " 0.007781446911394596,\n",
              " 0.0018133465200662613,\n",
              " 0.013083027675747871,\n",
              " 0.0009975226130336523,\n",
              " 0.0022324458695948124,\n",
              " 0.022287027910351753,\n",
              " 0.015844043344259262,\n",
              " 0.0047158789820969105,\n",
              " 0.009300341829657555,\n",
              " 0.0024686914402991533,\n",
              " 0.019743459299206734,\n",
              " 0.00926903635263443,\n",
              " 0.0051622893661260605,\n",
              " 0.007852322421967983,\n",
              " 0.003857468022033572,\n",
              " 0.006527322344481945,\n",
              " 0.02191348932683468,\n",
              " 0.007246880792081356,\n",
              " 0.005017302930355072,\n",
              " 0.0024913533125072718,\n",
              " 0.0030567627400159836,\n",
              " 0.01324125099927187,\n",
              " 0.0030317711643874645,\n",
              " 0.013037504628300667,\n",
              " 0.001632658182643354,\n",
              " 0.004542618058621883,\n",
              " 0.01026828121393919,\n",
              " 0.003980829380452633,\n",
              " 0.005136436782777309,\n",
              " 0.0042389086447656155,\n",
              " 0.00710904598236084,\n",
              " 0.004492798820137978,\n",
              " 0.012894578278064728,\n",
              " 0.013212908990681171,\n",
              " 0.0034672231413424015,\n",
              " 0.0019372167298570275,\n",
              " 0.010350572876632214,\n",
              " 0.006178190931677818,\n",
              " 0.009750377386808395,\n",
              " 0.00822135154157877,\n",
              " 0.020275909453630447,\n",
              " 0.0008047412848100066,\n",
              " 0.005732445046305656,\n",
              " 0.0029583428986370564,\n",
              " 0.013817844912409782,\n",
              " 0.00475687813013792,\n",
              " 0.00904049538075924,\n",
              " 0.0062666586600244045,\n",
              " 0.016883814707398415,\n",
              " 0.001163302338682115,\n",
              " 0.011859036982059479,\n",
              " 0.00391049962490797,\n",
              " 0.00645048264414072,\n",
              " 0.0029695951379835606,\n",
              " 0.00688166031613946,\n",
              " 0.0014648089418187737,\n",
              " 0.006955762859433889,\n",
              " 0.0009595701703801751,\n",
              " 0.00414413632825017,\n",
              " 0.003753099124878645,\n",
              " 0.003442278830334544,\n",
              " 0.008250038139522076,\n",
              " 0.00327116996049881,\n",
              " 0.003606329206377268,\n",
              " 0.005061107687652111,\n",
              " 0.008578593842685223,\n",
              " 0.0033931382931768894,\n",
              " 0.0048486823216080666,\n",
              " 0.0033263186924159527,\n",
              " 0.006852348335087299,\n",
              " 0.00386262615211308,\n",
              " 0.006245119962841272,\n",
              " 0.0036666046362370253,\n",
              " 0.0024297081399708986,\n",
              " 0.008486163802444935,\n",
              " 0.0054170237854123116,\n",
              " 0.0057805865071713924,\n",
              " 0.011554529890418053,\n",
              " 0.014802023768424988,\n",
              " 0.006148898508399725,\n",
              " 0.007288494613021612,\n",
              " 0.010330533608794212,\n",
              " 0.004093662369996309,\n",
              " 0.002765124198049307,\n",
              " 0.0014917750377207994,\n",
              " 0.002570162992924452,\n",
              " 0.004340613726526499,\n",
              " 0.00487400870770216,\n",
              " 0.010664070025086403,\n",
              " 0.020001154392957687,\n",
              " 0.008447403088212013,\n",
              " 0.0023181973956525326,\n",
              " 0.016778187826275826,\n",
              " 0.006487548351287842,\n",
              " 0.0010479455813765526,\n",
              " 0.0020976427476853132,\n",
              " 0.004202596377581358,\n",
              " 0.0035248056519776583,\n",
              " 0.004263848997652531,\n",
              " 0.0038326322101056576,\n",
              " 0.004876670893281698,\n",
              " 0.002999615855515003,\n",
              " 0.008258234709501266,\n",
              " 0.002015216974541545,\n",
              " 0.004328354727476835,\n",
              " 0.0016269693151116371,\n",
              " 0.005815633572638035,\n",
              " 0.00668268371373415,\n",
              " 0.0023990399204194546,\n",
              " 0.0002964666055049747,\n",
              " 0.003044264391064644,\n",
              " 0.0021162820048630238,\n",
              " 0.0035416078753769398,\n",
              " 0.003376811044290662,\n",
              " 0.004719615913927555,\n",
              " 0.0015030703507363796,\n",
              " 0.005082558840513229,\n",
              " 0.0019193710759282112,\n",
              " 0.006193937733769417,\n",
              " 0.0024260999634861946,\n",
              " 0.004791022278368473,\n",
              " 0.0034241992980241776,\n",
              " 0.001642198651097715,\n",
              " 0.0025943699292838573,\n",
              " 0.011455293744802475,\n",
              " 0.010567600838840008,\n",
              " 0.003525093663483858,\n",
              " 0.002572482917457819,\n",
              " 0.004374013748019934,\n",
              " 0.003853070782497525,\n",
              " 0.0037733365315943956,\n",
              " 0.003223059233278036,\n",
              " 0.0046039363369345665,\n",
              " 0.001948328921571374,\n",
              " 0.003394163679331541,\n",
              " 0.00397553201764822,\n",
              " 0.0026523047126829624,\n",
              " 0.0008921075495891273,\n",
              " 0.016860945150256157,\n",
              " 0.0038832600694149733,\n",
              " 0.004212930332869291,\n",
              " 0.017399601638317108,\n",
              " 0.0030158418230712414,\n",
              " 0.005411677993834019,\n",
              " 0.008870391175150871,\n",
              " 0.0033281357027590275,\n",
              " 0.0026242504827678204,\n",
              " 0.0036021540872752666,\n",
              " 0.0012341702822595835,\n",
              " 0.0008358623017556965,\n",
              " 0.0034988881088793278,\n",
              " 0.0028287875466048717,\n",
              " 0.0009135683067142963,\n",
              " 0.00861927680671215,\n",
              " 0.000827728770673275,\n",
              " 0.008574280887842178,\n",
              " 0.005998468492180109,\n",
              " 0.003498147474601865,\n",
              " 0.004852933343499899,\n",
              " 0.003107031574472785,\n",
              " 0.005128006916493177,\n",
              " 0.0049628159031271935,\n",
              " 0.005444932729005814,\n",
              " 0.0088045010343194,\n",
              " 0.001274643698707223,\n",
              " 0.0029789754189550877,\n",
              " 0.0024583854246884584,\n",
              " 0.0055956486612558365,\n",
              " 0.00946231372654438,\n",
              " 0.010428272187709808,\n",
              " 0.005956538952887058,\n",
              " 0.007751380559056997,\n",
              " 0.0022880244068801403,\n",
              " 0.0010951173026114702,\n",
              " 0.0013773790560662746,\n",
              " 0.0056334203109145164,\n",
              " 0.005399611312896013,\n",
              " 0.0042985607869923115,\n",
              " 0.0004221262352075428,\n",
              " 0.002018687082454562,\n",
              " 0.005756671540439129,\n",
              " 0.0013025085208937526,\n",
              " 0.0036405809223651886,\n",
              " 0.003352705854922533,\n",
              " 0.002487501362338662,\n",
              " 0.0016326192999258637,\n",
              " 0.0010198878590017557,\n",
              " 0.004065098240971565,\n",
              " 0.00047664408339187503,\n",
              " 0.00485115684568882,\n",
              " 0.0007927414262667298,\n",
              " 0.0020806328393518925,\n",
              " 0.0024827769957482815,\n",
              " 0.013570407405495644,\n",
              " 0.003950950689613819,\n",
              " 0.001429710304364562,\n",
              " 0.007278427481651306,\n",
              " 0.013358170166611671,\n",
              " 0.0035792775452136993,\n",
              " 0.0020017747301608324,\n",
              " 0.005549411289393902,\n",
              " 0.004663580097258091,\n",
              " 0.0029038707725703716,\n",
              " 0.003173707751557231,\n",
              " 0.002249064389616251,\n",
              " 0.0007920793141238391,\n",
              " 0.0032785888761281967,\n",
              " 0.014210822060704231,\n",
              " 0.004378047306090593,\n",
              " 0.0043013934046030045,\n",
              " 0.014174595475196838,\n",
              " 0.00993973109871149,\n",
              " 0.0038232181686908007,\n",
              " 0.0029245081823319197,\n",
              " 0.0027229231782257557,\n",
              " 0.0006897287676110864,\n",
              " 0.0036587812937796116,\n",
              " 0.0009472324745729566,\n",
              " 0.000331911607645452,\n",
              " 0.00262644374743104,\n",
              " 0.0016469440888613462,\n",
              " 0.004364078398793936,\n",
              " 0.006539816968142986,\n",
              " 0.0050125932320952415,\n",
              " 0.0044522457756102085,\n",
              " 0.0022758974228054285,\n",
              " 0.003056806279346347,\n",
              " 0.0043337480165064335,\n",
              " 0.002861239016056061,\n",
              " 0.0016655015060678124,\n",
              " 0.005424586124718189,\n",
              " 0.0024316932540386915,\n",
              " 0.0008515313384123147,\n",
              " 0.007450089789927006,\n",
              " 0.0025601487141102552,\n",
              " 0.0040351394563913345,\n",
              " 0.00526094064116478,\n",
              " 0.004059555474668741,\n",
              " 0.0005096130189485848,\n",
              " 0.001428332761861384,\n",
              " 0.0036135080736130476,\n",
              " 0.006888966076076031,\n",
              " 0.010586971417069435,\n",
              " 0.007455815561115742,\n",
              " 0.002803247421979904,\n",
              " 0.006565375719219446,\n",
              " 0.0032925859559327364,\n",
              " 0.0023822770453989506,\n",
              " 0.001998164923861623,\n",
              " 0.004672834184020758,\n",
              " 0.0010134601034224033,\n",
              " 0.002730844309553504,\n",
              " 0.011870777234435081,\n",
              " 0.002001762855798006,\n",
              " 0.0005493039498105645,\n",
              " 0.002568256575614214,\n",
              " 0.0036373857874423265,\n",
              " 0.0034640077501535416,\n",
              " 0.002799199428409338,\n",
              " 0.0025233400519937277,\n",
              " 0.0033856804948300123,\n",
              " 0.002932858420535922,\n",
              " 0.0015944575425237417,\n",
              " 0.0009881766745820642,\n",
              " 0.007642609067261219,\n",
              " 0.0033038672991096973,\n",
              " 0.002325208391994238,\n",
              " 0.005089779384434223,\n",
              " 0.007730891462415457,\n",
              " 0.00927991233766079,\n",
              " 0.008029053919017315,\n",
              " 0.0021745902486145496,\n",
              " 0.0016857401933521032,\n",
              " 0.0008164189639501274,\n",
              " 0.002044329419732094,\n",
              " 0.0004887885879725218,\n",
              " 0.009369788691401482,\n",
              " 0.0012667360715568066,\n",
              " 0.002061116974800825,\n",
              " 0.0042500062845647335,\n",
              " 0.003618939546868205,\n",
              " 0.0036457120440900326,\n",
              " 0.0071835205890238285,\n",
              " 0.006325894501060247,\n",
              " 0.0015683865640312433,\n",
              " 0.0008500486728735268,\n",
              " 0.007936790585517883,\n",
              " 0.003298801137134433,\n",
              " 0.003917451947927475,\n",
              " 0.010785980150103569,\n",
              " 0.00232295342721045,\n",
              " 0.0015744505217298865,\n",
              " 0.005607936065644026,\n",
              " 0.004470990505069494,\n",
              " 0.0026837221812456846,\n",
              " 0.0025026616640388966,\n",
              " 0.0016120722284540534,\n",
              " 0.004898491781204939,\n",
              " 0.005811585113406181,\n",
              " 0.002015485893934965,\n",
              " 0.012004965916275978,\n",
              " 0.007229062728583813,\n",
              " 0.003210383700206876,\n",
              " 0.0015414425870403647,\n",
              " 0.005340984556823969,\n",
              " 0.0011509384494274855,\n",
              " 0.0004290995711926371,\n",
              " 0.0053147925063967705,\n",
              " 0.013875879347324371,\n",
              " 0.002291242592036724,\n",
              " 0.00912463665008545,\n",
              " 0.006923081818968058,\n",
              " 0.006586548872292042,\n",
              " 0.002351165283471346,\n",
              " 0.002411970403045416,\n",
              " 0.003227983834221959,\n",
              " 0.00530257960781455,\n",
              " 0.0020939591340720654,\n",
              " 0.009194768033921719,\n",
              " 0.002268483629450202,\n",
              " 0.0013482847716659307,\n",
              " 0.0033037581015378237,\n",
              " 0.009744067676365376,\n",
              " 0.0008034453494474292,\n",
              " 0.0015302884858101606,\n",
              " 0.0060224877670407295,\n",
              " 0.002313025062903762,\n",
              " 0.003067572368308902,\n",
              " 0.0031410171650350094,\n",
              " 0.0028551232535392046,\n",
              " 0.001728144707158208,\n",
              " 0.0038079661317169666,\n",
              " 0.003065854776650667,\n",
              " 0.0036255684681236744,\n",
              " 0.0020653321407735348,\n",
              " 0.003602724988013506,\n",
              " 0.0029441057704389095,\n",
              " 0.004110076930373907,\n",
              " 0.002513214712962508,\n",
              " 0.0018226004904136062,\n",
              " 0.0015929481014609337,\n",
              " 0.0014280809555202723,\n",
              " 0.005155700258910656,\n",
              " 0.01378963328897953,\n",
              " 0.0031524288933724165,\n",
              " 0.0007438644533976912,\n",
              " 0.0007680378621444106,\n",
              " 0.000939115125220269,\n",
              " 0.004606969188898802,\n",
              " 0.0032533437479287386,\n",
              " 0.0004865294904448092,\n",
              " 0.0023076857905834913,\n",
              " 0.0021782992407679558,\n",
              " 0.0015488391509279609,\n",
              " 0.0008988699992187321,\n",
              " 0.008128346875309944,\n",
              " 0.004359367769211531,\n",
              " 0.004249350633472204,\n",
              " 0.001856677932664752,\n",
              " 0.0010620279936119914,\n",
              " 0.000813210557680577,\n",
              " 0.00909659918397665,\n",
              " 0.00244318600744009,\n",
              " 0.0014133378863334656,\n",
              " 0.0007648115279152989,\n",
              " 0.002227434189990163,\n",
              " 0.0028720670379698277,\n",
              " 0.0010323103051632643,\n",
              " 0.0024690188001841307,\n",
              " 0.003844511229544878,\n",
              " 0.0019112369045615196,\n",
              " 0.003502822248265147,\n",
              " 0.00217768969014287,\n",
              " 0.0024008455220609903,\n",
              " 0.0028595956973731518,\n",
              " 0.0011366598773747683,\n",
              " 0.0005132759106345475,\n",
              " 0.0008285387302748859,\n",
              " 0.0012301532551646233,\n",
              " 0.0009811536874622107,\n",
              " 0.0007066390826366842,\n",
              " 0.006219323258846998,\n",
              " 0.0006284349947236478,\n",
              " 0.002033176366239786,\n",
              " 0.0020244172774255276,\n",
              " 0.0027864379808306694,\n",
              " 0.0025553773157298565,\n",
              " 0.004216373898088932,\n",
              " 0.0027867003809660673,\n",
              " 0.0038902834057807922,\n",
              " 0.0011280422331765294,\n",
              " 0.005558903329074383,\n",
              " 0.0010624999413266778,\n",
              " 0.0028294138610363007,\n",
              " 0.0033483062870800495,\n",
              " 0.0033456652890890837,\n",
              " 0.0019474606961011887,\n",
              " 0.002055362332612276,\n",
              " 0.0035797073505818844,\n",
              " 0.0045092543587088585,\n",
              " 0.009191551245748997,\n",
              " 0.002444624435156584,\n",
              " 0.005106599535793066,\n",
              " 0.004346384666860104,\n",
              " 0.004416135139763355,\n",
              " 0.0035762314219027758,\n",
              " 0.0032516606152057648,\n",
              " 0.0021462666336447,\n",
              " 0.0064795734360814095,\n",
              " 0.0031995580065995455,\n",
              " 0.003719814121723175,\n",
              " 0.003617007751017809,\n",
              " 0.0016538299387320876,\n",
              " 0.002594445366412401,\n",
              " 0.0031309262849390507,\n",
              " 0.002355138538405299,\n",
              " 0.0013243920402601361,\n",
              " 0.008476527407765388,\n",
              " 0.0013411680702120066,\n",
              " 0.003134519560262561,\n",
              " 0.000591432151850313,\n",
              " 0.007319668773561716,\n",
              " 0.00033890240592882037,\n",
              " 0.0029797940514981747,\n",
              " 0.004400424659252167,\n",
              " 0.0009498869767412543,\n",
              " 0.0020794598385691643,\n",
              " 0.005757549777626991,\n",
              " 0.0016906323144212365,\n",
              " 0.0006695110932923853,\n",
              " 0.004721513949334621,\n",
              " 0.002037748461589217,\n",
              " 0.001551849883981049,\n",
              " 0.0007825969951227307,\n",
              " 0.00688727293163538,\n",
              " 0.01068748626857996,\n",
              " 0.0037850767839699984,\n",
              " 0.005678344052284956,\n",
              " 0.0058375014923512936,\n",
              " 0.004041764419525862,\n",
              " 0.001895378576591611,\n",
              " 0.00229101674631238,\n",
              " 0.0011935594957321882,\n",
              " 0.008834761567413807,\n",
              " 0.0024739233776926994,\n",
              " 0.006201072596013546,\n",
              " 0.0007865909719839692,\n",
              " 0.000611082767136395,\n",
              " 0.0019028850365430117,\n",
              " 0.003429752541705966,\n",
              " 0.00043118110625073314,\n",
              " 0.001450235489755869,\n",
              " 0.004035355523228645,\n",
              " 0.0012982059270143509,\n",
              " 0.001219326863065362,\n",
              " 0.0008845819975249469,\n",
              " 0.0018183887004852295,\n",
              " 0.0016049537807703018,\n",
              " 0.005034417845308781,\n",
              " 0.0044678980484604836,\n",
              " 0.0029803516808897257,\n",
              " 0.0038044601678848267,\n",
              " 0.0014239813899621367,\n",
              " 0.0009359439136460423,\n",
              " 0.005740675143897533,\n",
              " 0.002993206959217787,\n",
              " 0.0020013395696878433,\n",
              " 0.0014386557741090655,\n",
              " 0.003189731389284134,\n",
              " 0.0017761830240488052,\n",
              " 0.005510664079338312,\n",
              " 0.0017536969389766455,\n",
              " 0.0017693864647299051,\n",
              " 0.0011521612759679556,\n",
              " 0.00036047064349986613,\n",
              " 0.0020569879561662674,\n",
              " 0.008137494325637817,\n",
              " 0.0063722580671310425,\n",
              " 0.003361334325745702,\n",
              " 0.0015480152796953917,\n",
              " 0.0003963812196161598,\n",
              " 0.001693479367531836,\n",
              " 0.0011326717212796211,\n",
              " 0.0010284020099788904,\n",
              " 0.0026050296146422625,\n",
              " 0.005946340970695019,\n",
              " 0.0009595653973519802,\n",
              " 0.0024523260071873665,\n",
              " 0.002336908131837845,\n",
              " 0.001636290573514998,\n",
              " 0.00046807751641608775,\n",
              " 0.0006379297119565308,\n",
              " 0.0012634160229936242,\n",
              " 0.001361005357466638,\n",
              " 0.0020055898930877447,\n",
              " 0.003253054339438677,\n",
              " 0.002470832783728838,\n",
              " 0.0008260499453172088,\n",
              " 0.0006313041667453945,\n",
              " 0.003444779198616743,\n",
              " 0.0016440131003037095,\n",
              " 0.0009707165881991386,\n",
              " 0.0003072306571993977,\n",
              " 0.0009485490736551583,\n",
              " 0.0012398151447996497,\n",
              " 0.0014490141766145825,\n",
              " 0.004821169190108776,\n",
              " 0.0054077343083918095,\n",
              " 0.0018058521673083305,\n",
              " 0.0034053842537105083,\n",
              " 0.004073249641805887,\n",
              " 0.00823896937072277,\n",
              " 0.0037724010180681944,\n",
              " 0.004387805704027414,\n",
              " 0.003626537974923849,\n",
              " 0.0004449539410416037,\n",
              " 0.0036572315730154514,\n",
              " 0.0032018783967942,\n",
              " 0.0010237634414806962,\n",
              " 0.006052403245121241,\n",
              " 0.002891326555982232,\n",
              " 0.003255395218729973,\n",
              " 0.004650275222957134,\n",
              " 0.003334048669785261,\n",
              " 0.0012295277556404471,\n",
              " 0.0009541053441353142,\n",
              " 0.0015187922399491072,\n",
              " 0.0015683515230193734,\n",
              " 0.0009369069593958557,\n",
              " 0.0015346517320722342,\n",
              " 0.0017783604562282562,\n",
              " 0.01205313391983509,\n",
              " 0.00477178767323494,\n",
              " 0.0039110989309847355,\n",
              " 0.002916885307058692,\n",
              " 0.0005406324053183198,\n",
              " 0.0011701255571097136,\n",
              " 0.005930804181843996,\n",
              " 0.0018954317783936858,\n",
              " 0.0041317930445075035,\n",
              " 0.004125350620597601,\n",
              " 0.0018019042909145355,\n",
              " 0.0014338655164465308,\n",
              " 0.004195649642497301,\n",
              " 0.002007960807532072,\n",
              " 0.0015170390252023935,\n",
              " 0.00587161909788847,\n",
              " 0.0013736130204051733,\n",
              " 0.00130773417185992,\n",
              " 0.00720538105815649,\n",
              " 0.0008876632200554013,\n",
              " 0.002105745952576399,\n",
              " 0.006991781294345856,\n",
              " 0.0009561732294969261,\n",
              " 0.001064131734892726,\n",
              " 0.001228078966960311,\n",
              " 0.0020315535366535187,\n",
              " 0.0031291525810956955,\n",
              " 0.001952902413904667,\n",
              " 0.00405783299356699,\n",
              " 0.003215911565348506,\n",
              " 0.001096986117772758,\n",
              " 0.003311463166028261,\n",
              " 0.0006178974290378392,\n",
              " 0.0007013567956164479,\n",
              " 0.0019314255332574248,\n",
              " 0.0014856916386634111,\n",
              " 0.0004080917569808662,\n",
              " 0.0023074939381331205,\n",
              " 0.0019735274836421013,\n",
              " 0.0030403039418160915,\n",
              " 0.0004686954489443451,\n",
              " 0.00043449277291074395,\n",
              " 0.0009691837476566434,\n",
              " 0.003072718856856227,\n",
              " 0.003496094373986125,\n",
              " 0.007257434073835611,\n",
              " 0.0022238888777792454,\n",
              " 0.005501709878444672,\n",
              " 0.01279396377503872,\n",
              " 0.00047082515084184706,\n",
              " 0.0020965267904102802,\n",
              " 0.000983147183433175,\n",
              " 0.001234198221936822,\n",
              " 0.0007970159058459103,\n",
              " 0.00038438820047304034,\n",
              " 0.0020240554586052895,\n",
              " 0.0017985461745411158,\n",
              " 0.0013154157204553485]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'Model1.pt')"
      ],
      "metadata": {
        "id": "-hhIzDeaOs3h"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class x_transform(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack1 = nn.Sequential(\n",
        "            nn.Linear(n_feat, 2 * n_feat),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * n_feat, 2 * n_feat),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * n_feat, n_feat)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.linear_relu_stack1(x)\n",
        "        return x\n",
        "\n",
        "model = x_transform()"
      ],
      "metadata": {
        "id": "uNjNYgKxTQmJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('Model1.pt', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYeraZNdThn-",
        "outputId": "8e0a719f-a39c-405b-c1ad-27e2704d2202"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train).to(torch.float32)\n",
        "X_test = torch.tensor(X_test).to(torch.float32)"
      ],
      "metadata": {
        "id": "e6XPRB3eTq5v"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = model(X_train)\n",
        "X_test = model(X_test)"
      ],
      "metadata": {
        "id": "85b_rAXqTtwq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.detach().numpy()\n",
        "X_test = X_test.detach().numpy()\n",
        "\n",
        "qsvc = SVC(kernel=adhoc_kernel.evaluate)\n",
        "\n",
        "qsvc.fit(X_train, Y_train)\n",
        "score = qsvc.score(X_test, Y_test)\n",
        "print(score)\n",
        "print(qsvc.predict(X_test))\n",
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjgirUYoTvyX",
        "outputId": "c1205f39-68b9-48c4-9725-f9f3a630c1f8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n",
            "[1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0]\n",
            "[1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ad_KYVCSTzbw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}